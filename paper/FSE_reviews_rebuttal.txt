
We sincerely hope that reviewers can carefully read this rebuttal as some comments revealed important misconceptions.

R1: “manually capture the options from the example attack to form the seed rule”

The first step is an automatic extraction of options from the attack message, generating the seed rule. The option-field mapping was written manually, but as this is protocol information, there’s no need for rewriting it.

R1: “- the topic is not very relevant”

Please consider that (a) Intrusion Detection is a multi-million dollar business (type “IDS solutions” on Google), (b) detecting intrusion with rule-based IDS is the most commonly-adopted solution for intrusion detection in industry, and (c) generating these rules is a very common activity in this business. We explained this on intro, providing statistics. 

R1: on bias toward existing rules. (...In my opinion, as a rule learner, it is better to learn new rules.)

This comment was unclear to us. The goal of Syrius is to create new rules. Rules that were not seen before to help the Suricata/Snort community. This goal is reflected in our experimental setup.

R1 and R2: approach is not general enough

As mentioned above, synthesizing rules is a challenging problem. Our goal is to handle most cases--and we provided statistical data showing that we do handle. The more rules that can be generated the better. It is unclear to us how general is “general enough” (as per comment), especially considering the fundamental limitation of existing learning algorithms. 

R2: “...[Syrius] does not guarantee correctness, which is needed in the context of IDS.”

Even manual generation of rules can produce rules with false positives and negatives.  Classical ML-based approaches for Anomaly-based Intrusion Detection suffer from this problem the same way we do. Having said that, Syrius does guarantee correctness (as per false positives and negatives counts) relative to the very large dataset we use.

R2. “...what happens if the set of common options is empty? “ How the threshold would help in this case?”

If the set of common options is empty, the input is invalid for Syrius, and the threshold option is not suitable, since this option aims to count the occurrences of packets that have common characteristics.

R2) How do you define N in line 6 of algorithm 1? Why not considering all possible rules that canoe generated from the seed rule?

Using smaller values for N is a choice purely computational. Making N=number_of_options would find all possible rules, but also would take more time. We chose 2 as value for N as it was enough for finding the “Golden Rule”.

R2 and R3: “...No empirical comparison with other existing methods.”
R3: “...the authors could apply their own system to the same dataset(s) and compare to the published results.”

We admit this is a problem. We could not access alternative tools. We think that to fairly evaluate our tool against others we would need to access and run alternative tools. This would enable us to confirm that these tools actually produce the results they report.

R3. on Significance “Much of the recent research in this area applies real machine learning to NIDS and generates classifiers for malicious vs. benign.”

We know. We explained the rationale for using a non-traditional “learning” mechanism. Existing ML approaches deal well with numerical and categorical data, but fail to handle structured and string data, which are central to rule synthesis. Packets contents (the most common option in rules) can potentially have an infinite number of permutations [https://arxiv.org/pdf/1903.12101]. Anomaly-based NIDS monitor features like ratio of accesses by a given protocol that can be modeled with numerical features. Rule-base NIDS are complementary to anomaly-based NIDS.

R3. “...Please explain this, I don't understand how 1010 maps to RM”

typos:
lines 207, 214, 215: flagbits -> fragbits
line 213: 1010 -> 101

The fragbits option maps each bit of the Flags field to a character M, D or R, where M represents the most and R the least significant. So 101 means that the bits M and R are set and that the bit D is not, therefore the fragbits value is MR (or RM).

-------------------------------------
Dear Marcelo d'Amorim,

Thank you for your submission to ESEC/FSE 2020. The HotCrp site is now open
for rebuttal (https://esecfse2020.hotcrp.com) The rebuttal site will be
open until April 30th, 23:59 AoE.

During this time, you will have access to the current state of your reviews
and have the opportunity to submit a response of up to 750 words. This is a
soft limit, you may submit longer responses, however, reviewers are not
required to read beyond the 750 word limit.

Please keep in mind the following during this process:

* The response must focus on any factual errors in the reviews and any
questions posed by the reviewers. It must not provide new research results
or reformulate the presentation. Try to be as concise and to the point as
possible.

* The rebuttal period is an opportunity to react to the reviews, but not a
requirement to do so. Thus, if you feel the reviews are accurate and the
reviewers have not asked any questions, then you do not have to respond.

* The reviews are submitted by the PC members, without any coordination
between them. Thus, there may be inconsistencies. Furthermore, these are
not the final versions of the reviews. The reviews can be updated later to
take into account the discussions among the program committee, and we may
find it necessary to solicit additional reviews after the rebuttal period.

* Your response will be seen by all PC members who have access to the
discussion of your paper, so please try to be polite and constructive.

The reviews and scores for your paper are attached to this email. To submit
your response you should log on the HotCrp website and select your
submission on the menu. If you decide to withdraw your paper, please edit
the submission in HotCrp to mark the paper as withdrawn.

       Title: Learning to Synthesize Rules for Rule-based Intrusion
              Detectors
       Site: https://esecfse2020.hotcrp.com/paper/909

Best Regards,
Myra Cohen and Tom Zimmermann

--
Review #909A
===========================================================================

Overall merit
-------------
2. Weak reject

Paper summary
-------------
This paper proposes to learn the rules for rule based intrusion detectors. It proposes a three step process for this purpose: 1) manually capture the options from the example attack to form the seed rule; 2) create the variants of the seed rule by doing random mutations starting from the seed rule; and 3) ranking the obtained variants. The benign examples are used filter the negative examples in step 2 for making the mutation minimal. Some experiments have been conducted for the evaluation.

Strengths and weaknesses
------------------------
Strengths:
- the process is complete
- have used the well-known attack database

Weaknesses:
- the topic is not very relevant
- it does not seem to be a general method, the learning process is case by case and seems to depend heavily on the manual operation step.

Comments for the authors
------------------------
Soundness:
This paper introduces a three-step process for learning the rules for the rule-based intrusion detectors. But there is no methodological guidance for this process. For example, the first step is manually extracting the options from the attack message. There is not any guidance for the option extraction. In particular, the seed rule extracted by this process will directly affect the rules generated in the second step. Another questions about the soundness are the heuristics for ranking the generated rules. First, why the similarity between the generated rule and the existing rule is a suitable criteria? Is this more biased towards existing rules than newly discovered ones? Is it reasonable to define similarity as the four indicators in the section 4.3?

Significance:
As mentioned before, the ranking strategy proposed in this paper seems to be more biased towards learning existing rules. Is it right? In my opinion, as a rule learner, it is better to learn new rules. Learning new rules  might be more valuable and contributes more to the community. Of course, the newly-learning rule needs to be confirmed later.

Novelty:
There are some other strategy that the idea is similar to this work, e.g. genetic algorithm based rule learning. It seems having larger search space for rule learning than the proposal in this paper.

Recoverability, Replicability and Reproducibility:
As mentioned before, the learning result depends on the manual extraction of the seed rule. And also, it does not tell the formulation of the input message and fails to tell how to extract the necessary information from the input.

Presentation:
The presentation is too detailed. The operation process is case-by-case. It lacks of refinement, generalization,  and formalization.


* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *


Review #909B
===========================================================================

Overall merit
-------------
2. Weak reject

Paper summary
-------------
The paper proposes a method to learn rules for a rule-based network intrusion detection system (NIDS). The method is a pipeline of three main steps: reverse engineer of an over-specified rule from a given example of attack, then synthesis new rules that are possibly more general but at he same time are guaranteed to reflect benign behaviour, i.e. not to have false positives. The final set of constructed rules, referred to as plausible rules, are then ranked using heuristics that compare the level of similarity between each of these rule with ground truth rules defined by humans. Experimental results show that the proposed approach is capable of ranking 70% of the time the correct rule among top five plausible rules.

Strengths and weaknesses
------------------------
Strength:
+ well written paper with a clear simplified example at the beginning
+ experimental results show advantage of the approach

Weaknesses:
- The approach is not general enough
- Use of heuristics does not guarantee correctness, which is needed in the context of IDS.
- Heuristics are dataset dependent
- No empirical comparison with other existing methods.

Comments for the authors
------------------------
The problem of network intrusion detection is very relevant in the SE community and automated learning of intrusion detection rules is an open problem. The paper is well written and self-contained, with a very clear illustrative example that successfully introduce the main steps of the approach. I have, however, various concerns which stop me from recommending acceptance.

The approach is more a form of rule synthesis than learning. I would change the title of the paper to “Rule Synthesis for Rule-Based Intrusion Detection” and remove the word “learning” as the learning process is in tshie case very minimal.

Firstly, the authors state that the input to the approach is a set of examples of attacks. The rule creation algorithm however starts from a over-specialised rule that is reverse engineered from a single attack example. So it is not clear how the other maligned attack examples (mTrs) are used during the generalisation process. It is, on the other hand, clear how the benign traffics (bTrs) are used during the second phase of the approach.  Regarding the novelty of the algorithm, the idea of creating rules starting from an over-specialised rule is very similar to what well established symbolic rule learning methods have proposed in the literature. The “rule creation” step would correspond to a process of generalisation which, in the context of symbolic rule learning, guarantee to be sound and able to learn the most general rule, instead of an approximating it through heuristics as proposed in this paper. I don’t really see the advantage of using this approach instead of well known sound symbolic rule learning methods. Examples of symbolic rule learning approaches are those algorithms and systems that follow the “inverse entailment” learning method [1].

As for the significance, the heuristics seem to be dependent on the type of dataset and positive and negative examples considered by the algorithm. This makes the approach even more dataset specific. Changing type of datasets might require changing the heuristics as there is no evidence to believe that the approach will perform well. The authors claim that the approach is applicable to any IDS type, including Snort. It would’ve been good to show that indeed this is the case. The lack of empirical evaluation on this important aspect of the work make the significance of this work very limited.

As for the soundness:

- the outcomes of the algorithm is a rank of rules. This is not necessarily ideal as in practice an IDS has to use specific rules that correctly recognise attacks. Related to my first point, the metric used for evaluating the outcomes is not in practice very useful. The results in Table 3 seem to show that in the case of very limited number of variation of an attack and high number of seed options many plausible rules are generated and the correct rule results to be quite low in the ranking.

- although I do realise that starting the creation of rules from an over-specialised rule is computationally better than starting from just the attack example, the number of plausible rules does grow wrt the number of options included in the seed rule, affecting the level of ranking of the correct rules.

- it is also not clear if the “second step” of the algorithm works though a "tree of plausible rules" or a “lattice” of plausible rules. They can be many “incomparable” plausible rules in terms of number of options used as conditions.

- It is also not clear how the value of “N” is picked in the algorithm and if the algorithm does indeed expire all the possible plausible rules.

-  I’m not sure if the proof of Proposition 4.1 captures the statement of the Proposition. It seems to be stating the opposite of what has been defined to be the output of the algorithm.

Reference:
[1] “Inverse Entailment and Progol”, S. Muggleton, New Generation Computing 13 (1995), 245-286.   

Typo:
1) Include in Table 1 the description for ttl.
2) Line 207: use “flag” instead of “flagbits’ to match with the name given in Table  or change the name in Table 1.
3) Line 505: Raking —> Ranking

Questions for the authors’ response
-------------------------------------
1) On page 4, line 396 the authors state “Syrius runs the same approach for each message and identifies common options across all messages”. But what happens if the set of common options is empty? How the threshold would help in this case?

2) How do you define N in line 6 of algorithm 1? Why not considering all possible rules that canoe generated from the seed rule?


* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *


Review #909C
===========================================================================

Overall merit
-------------
1. Reject

Paper summary
-------------
The paper proposes an approach to generating rules for signature-based network intrusion detection systems (NIDS).  Since there are relatively few examples of attacks relative to benign messages, the authors claim that machine learning is not applicable. So, despite the paper's title, the approach does not involve 'learning' in the machine learning sense.  Instead, the tool (called Syrius) goes through three stages: use the available attack messages to create an overly specific seed rule; use benign messages to derive a set of rules from that seed that still capture the attack but do not capture the benign messages; rank the set of rules according to their 'similarity' to human-written rules using heuristics such as frequently using those options that occur most commonly in human-written rules (options generally correspond to fields in network packets or content patterns in http payloads).  The rules in this case are targeted to Suricata, an open-source NIDS, but the approach is said to be applicable to other NIDS where a large number of sample human-written rules are available.

Strengths and weaknesses
------------------------
strengths:

The three-stage pipeline is an interesting approach.

The research questions are good (although they did not go far enough, as noted below there is no experimental comparison to alternative techniques).

It is good that the paper includes an example where Syrius performed poorly (section 6).  I wish more papers included such examples!

weaknesses:

There's been a lot of research on automatically generating rules for signature-based network intrusion detection, but the paper cites only three relevant papers, references 30, 33 and 35, the most recent of which is from 2015.  Some examples of more recent related work that generates rules: https://arxiv.org/abs/1903.12101, https://ieeexplore.ieee.org/abstract/document/7585840, https://ieeexplore.ieee.org/abstract/document/7544894. 

The experimental evaluation (section 5) does not compare Syrius directly to any other work. The paper says that the implementations of related work are not available (1088-1090). However, much of the research in this area reports evaluation results on public datasets, so even when the tool implementation is not available, the authors could apply their own system to the same dataset(s) and compare to the published results.

The title and body of the paper often use the terms 'learning' and 'learn' in a misleading way, since the technical details in section 4 do not involve machine learning.

Comments for the authors
------------------------
soundness:

The heuristics used for ranking candidate rules (section 4.3) are not well-justified, they seem to be chosen based on what is easy to calculate.  This is mitigated to some extent by RQ2.

"Considering construct validity, the choice of metric used to evaluate the approach may not be applicable in practice. We considering the use of ranking of the golden rule a reasonable choice as the approach places the golden rule close to the top in most cases." (lines 1023-1028) This is a self-serving argument, not an appropriate mitigation.

significance:

Much of the recent research in this area applies real machine learning to NIDS and generates classifiers for malicious vs. benign.  (There is also a lot of work on embodying NIDS in FPGA, since software NIDS is deemed too slow.)

novelty:

As noted in "weaknesses" above, there are other techniques that may be as good or better for generating rules for signature-based NIDS, but the paper does not evaluate in comparison.

recoverability, replicability, reproducibility:

A public implementation is available by request.  The authors should post a public link in the final version of the paper. 

The dataset is already publicly available.

presentation:

Section 3 should include the actual generated seed rule and the 9 generated plausible rules, in the generated rank order, for the PingScan example. The authors could also add some examples of discarded rules.

The first part of section 4 repeats some of the material in section 3.

The term "golden rule", used throughout the paper, is not defined until section 5.

other:

The paper has no keywords.

It is odd that 25 of the paper's 36 references are to (non-research) websites. Many of these might be better provided as footnotes rather than cites.   

typos:

line 505: Raking -> Ranking

line 748: sensible -> sensitive

line 1018: is -> are

Questions for the authors’ response
-------------------------------------
"For example, if the “Flags” field stores the value 1010, the corresponding rule option will be flagbits:RM, as each position of the vector encodes a different character in the flagbits rule option." (lines 211-215) Please explain this, I don't understand how 1010 maps to RM.


